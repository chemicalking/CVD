import streamlit as st
import altair as alt
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib import font_manager
import matplotlib as mpl
#pip freeze > requirements.txt
# activate finlab
# cd "C:\Users\USER\Desktop\finlab_course2019\streamlit\09_app_multipage"
# streamlit run 01_ğŸˆ_main_app.py

st.set_page_config(
    page_title="NF3 æ°£é«”æµé‡èˆ‡ä½¿ç”¨å„€è¡¨æ¿",
    page_icon="â„ï¸",
    layout="wide",
    initial_sidebar_state="expanded")
alt.themes.enable("dark")

pd.set_option("styler.render.max_elements", 15346088)  # è¨­ç½®æœ€å¤§æ¸²æŸ“å…ƒç´ 

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ç¢ºä¿å­—é«”å¯ç”¨
def setup_chinese_font():
    try:
        # å˜—è©¦ä½¿ç”¨æ­£é»‘é«”
        font_path = 'C:/Windows/Fonts/msjh.ttc'  # æ­£é»‘é«”è·¯å¾‘
        font_prop = font_manager.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
    except:
        try:
            # å˜—è©¦ä½¿ç”¨å¾®è»Ÿé›…é»‘
            font_path = 'C:/Windows/Fonts/msyh.ttc'  # å¾®è»Ÿé›…é»‘è·¯å¾‘
            font_prop = font_manager.FontProperties(fname=font_path)
            plt.rcParams['font.family'] = font_prop.get_name()
        except:
            # å¦‚æœéƒ½å¤±æ•—ï¼Œä½¿ç”¨ç³»çµ±é»˜èªä¸­æ–‡å­—é«”
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']

setup_chinese_font()

# è¼‰å…¥è³‡æ–™
data = pd.read_csv(r"D:\curso\streamlit\09_app_multipage\data\Merged_Data.csv")
# æ–°å¢ LAYER æ¬„ä½
data['LAYER'] = data['RECIPEID'].str.extract('(BP|PFA|AS)')
# å°‡ TSTAMP è½‰æ›ç‚ºæ—¥æœŸæ™‚é–“æ ¼å¼ä»¥ä¾¿è™•ç†
data['TSTAMP'] = pd.to_datetime(data['TSTAMP'], format='%Y%m%d%H')

# å®šç¾©å–®ä½è½‰æ›å¸¸æ•¸
SCCM_TO_KG = 3.04 / (1000 * 60)  # è½‰æ›ç‚º kg/sï¼ŒNF3 å¯†åº¦ 3.04 kg/mÂ³
SCCM_TO_LITER = 1 / 60000  # è½‰æ›ç‚º l/s
SCCM_TO_SCCM = 1  # å–®ä½ä¿æŒä¸è®Š



# å®šç¾©ä½¿ç”¨ IQR æª¢æ¸¬é›¢ç¾¤å€¼çš„å‡½æ•¸
def detect_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[col] < lower_bound) | (df[col] > upper_bound)]

# è¨ˆç®— RPSC_GLASSID æ•¸é‡
def calculate_rpsc_glassid(data):
    rpsc_data = data[data['RECIPEID'].str.startswith('RPSC', na=False)]
    rpsc_glassid_count = rpsc_data['GLASSID'].nunique()
    return rpsc_glassid_count

# Streamlit æ‡‰ç”¨ç¨‹å¼
def main():
    st.title("NF3 æ°£é«”æµé‡èˆ‡ç”¨é‡å„€è¡¨æ¿")

    # é¸æ“‡ç´¯ç©æµé‡çš„å–®ä½
    st.sidebar.header("é¸æ“‡ç´¯ç©æµé‡çš„å–®ä½")
    flow_unit = st.sidebar.radio("å–®ä½:", ("kg/s", "l/s", "sccm"))

    # ç¢ºå®šå–®ä½è½‰æ›ä¿‚æ•¸èˆ‡æ¨™ç±¤
    if flow_unit == "kg/s":
        unit_conversion = SCCM_TO_KG
        unit_label = "(kg/s)"
    elif flow_unit == "l/s":
        unit_conversion = SCCM_TO_LITER
        unit_label = "(l/s)"
    else:
        unit_conversion = SCCM_TO_SCCM
        unit_label = "(sccm)"

    # æ–°å¢æ™‚é–“é¸é …
    st.sidebar.header("é¸æ“‡æ™‚é–“ç¶­åº¦")
    time_granularity = st.sidebar.radio("ç¶­åº¦:", ("å¹´", "å­£", "æœˆ", "é€±", "æ—¥"))

    # é¡¯ç¤ºå–®ä½è½‰æ›é‚è¼¯
    st.sidebar.markdown("### å–®ä½è½‰æ›èªªæ˜")
    st.sidebar.markdown(
        """
        - **SCCM to KG/S**: \( \text{[NF3_total_Flow]} \times 3.04 / (1000 \times 60) \)
        - **SCCM to L/S**: \( \text{[NF3_total_Flow]} / 60000 \)
        - **SCCM to SCCM**: å–®ä½ä¿æŒä¸è®Š
        """
    )

    # å´é‚Šæ¬„ç¯©é¸æ¢ä»¶
    st.sidebar.header("ç¯©é¸é¸é …")
    # å¢åŠ å…¨é¸é¸é …
    def add_all_option(options):
        """ç‚ºç¯©é¸å™¨æ·»åŠ å…¨é¸åŠŸèƒ½ã€‚"""
        return ["å…¨é¸"] + options

    # åˆå§‹åŒ–ç¯©é¸å™¨
    chamberid_filter = st.sidebar.multiselect("é¸æ“‡ CHAMBERID:", add_all_option(data['CHAMBERID'].dropna().unique().tolist()), default=["å…¨é¸"])
    toolid_filter = st.sidebar.multiselect("é¸æ“‡ TOOLID:", add_all_option(data['TOOLID'].dropna().unique().tolist()), default=["å…¨é¸"])
    recipeid_filter = st.sidebar.multiselect("é¸æ“‡ RECIPEID:", add_all_option(data['RECIPEID'].dropna().unique().tolist()), default=["å…¨é¸"])
    operation_filter = st.sidebar.multiselect("é¸æ“‡ OPERATION:", add_all_option(data['OPERATION'].dropna().unique().tolist()), default=["å…¨é¸"])
    product_filter = st.sidebar.multiselect("é¸æ“‡ PRODUCT:", add_all_option(data['PRODUCT'].dropna().unique().tolist()), default=["å…¨é¸"])
    chamber_code_filter = st.sidebar.multiselect("é¸æ“‡ CHAMBER_CODE:", add_all_option(data['CHAMBER_CODE'].dropna().unique().tolist()), default=["å…¨é¸"])
    sin_filter = st.sidebar.multiselect("é¸æ“‡ SIN:", add_all_option(data['SIN'].dropna().unique().tolist()), default=["å…¨é¸"])
    layer_filter = st.sidebar.multiselect("é¸æ“‡ LAYER:", add_all_option(data['LAYER'].dropna().unique().tolist()), default=["å…¨é¸"])
    step_name_filter = st.sidebar.multiselect("é¸æ“‡ STEP_NAME:", add_all_option(data['step_name'].dropna().unique().tolist()), default=['CLN1', 'CLN2', 'CLN3'], key="step_name")

    # ç¯©é¸æ•¸æ“š
    filtered_data = data.copy()
    if "å…¨é¸" not in chamberid_filter:
        filtered_data = filtered_data[filtered_data['CHAMBERID'].isin(chamberid_filter)]
    if "å…¨é¸" not in toolid_filter:
        filtered_data = filtered_data[filtered_data['TOOLID'].isin(toolid_filter)]
    if "å…¨é¸" not in recipeid_filter:
        filtered_data = filtered_data[filtered_data['RECIPEID'].isin(recipeid_filter)]
    if "å…¨é¸" not in operation_filter:
        filtered_data = filtered_data[filtered_data['OPERATION'].isin(operation_filter)]
    if "å…¨é¸" not in product_filter:
        filtered_data = filtered_data[filtered_data['PRODUCT'].isin(product_filter)]
    if "å…¨é¸" not in chamber_code_filter:
        filtered_data = filtered_data[filtered_data['CHAMBER_CODE'].isin(chamber_code_filter)]
    if "å…¨é¸" not in sin_filter:
        filtered_data = filtered_data[filtered_data['SIN'].isin(sin_filter)]
    if "å…¨é¸" not in layer_filter:
        filtered_data = filtered_data[filtered_data['LAYER'].isin(layer_filter)]
    if "å…¨é¸" not in step_name_filter:
        filtered_data = filtered_data[filtered_data['step_name'].isin(step_name_filter)]

    # è¨ˆç®—æ¯æ—¥æµé‡ä¸¦æª¢æ¸¬ç•°å¸¸å€¼
    daily_flow = filtered_data.groupby(filtered_data['TSTAMP'].dt.date)['NF3_total_Flow'].sum().reset_index()
    daily_flow.columns = ['Date', 'Daily_Flow']
    daily_flow['Daily_Flow'] *= unit_conversion

    # è¨ˆç®— 7 å¤©å’Œ 30 å¤©ç§»å‹•å¹³å‡ç·š
    daily_flow['7-Day MA'] = daily_flow['Daily_Flow'].rolling(window=7, min_periods=1).mean()
    daily_flow['30-Day MA'] = daily_flow['Daily_Flow'].rolling(window=30, min_periods=1).mean()

    # éæ¿¾æ‰ç•°å¸¸å€¼çš„æ¯æ—¥æµé‡
    Q1 = daily_flow['Daily_Flow'].quantile(0.25)
    Q3 = daily_flow['Daily_Flow'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    filtered_daily_flow = daily_flow[(daily_flow['Daily_Flow'] >= lower_bound) & (daily_flow['Daily_Flow'] <= upper_bound)]

    # ç¹ªè£½æ¯æ—¥ç”¨é‡èˆ‡ç§»å‹•å¹³å‡ç·šåœ–è¡¨
    fig, ax1 = plt.subplots(figsize=(12, 8))
    # ä½¿ç”¨éæ¿¾å¾Œçš„æ¯æ—¥æµé‡è¨ˆç®—ç´¯ç©æµé‡ï¼Œæ¯æœˆ1è™Ÿé‡ç½®
    filtered_daily_flow['Month'] = pd.to_datetime(filtered_daily_flow['Date']).dt.to_period('M')
    filtered_daily_flow['Cumulative_Flow'] = filtered_daily_flow.groupby('Month')['Daily_Flow'].cumsum()

    # è¨ˆç®— RPSC_GLASSID æ•¸é‡
    rpsc_glassid_count = calculate_rpsc_glassid(filtered_data)

    # é¡¯ç¤º RPSC_GLASSID è¨ˆç®—é‚è¼¯
    st.markdown("### RPSC_GLASSID è¨ˆç®—é‚è¼¯")
    st.markdown(
        """
        - **ç¯©é¸æ¢ä»¶**: é¸æ“‡ `RECIPEID` é–‹é ­ç‚º 'RPSC' çš„åˆ—
        - **å”¯ä¸€è¨ˆæ•¸**: è¨ˆç®— `GLASSID` æ¬„ä½ä¸­çš„å”¯ä¸€å€¼
        """
    )

    # é¡¯ç¤ºæ•¸æ“šæ‘˜è¦
    st.header("æ•¸æ“šæ‘˜è¦")
    st.write(filtered_data.describe())
    st.subheader(f"RPSC_GLASSID æ•¸é‡: {rpsc_glassid_count}")

    # æª¢æ¸¬ç•°å¸¸å€¼
    daily_outliers = detect_outliers(filtered_daily_flow, 'Daily_Flow')

    # é¡¯ç¤ºæ¯æ—¥æµé‡çš„ç•°å¸¸å€¼
    st.header("æ¯æ—¥æµé‡ç•°å¸¸å€¼")
    if not daily_outliers.empty:
        st.write("ä»¥ä¸‹ç•°å¸¸å€¼è¢«æª¢æ¸¬åˆ°:")
        st.dataframe(daily_outliers)
    else:
        st.write("æ¯æ—¥æµé‡æœªæª¢æ¸¬åˆ°ç•°å¸¸å€¼ã€‚")

    # é¡¯ç¤ºè¿‘ä¸‰åæ—¥æ•¸æ“šè¡¨æ ¼
    st.header("æœ€è¿‘ 30 å¤©æ•¸æ“šç´€éŒ„")

    # æå–æœ€è¿‘ 30 å¤©æ•¸æ“š
    recent_30_days = filtered_daily_flow.tail(30).copy()
    recent_30_days['Daily_Flow_kg'] = (recent_30_days['Daily_Flow'] * SCCM_TO_KG).round(2)
    recent_30_days['Daily_Flow_l'] = (recent_30_days['Daily_Flow'] * SCCM_TO_LITER).round(2)
    recent_30_days['Daily_Flow_sccm'] = (recent_30_days['Daily_Flow'] * SCCM_TO_SCCM).round(2)
    recent_30_days['Cumulative_Flow_converted'] = (recent_30_days['Cumulative_Flow'] * unit_conversion).round(2)

    # èª¿æ•´è¡¨æ ¼é¡¯ç¤ºæ ¼å¼ï¼Œæ—¥æœŸæ°´å¹³æ’åˆ—ï¼Œæœ€æ–°æ—¥æœŸåœ¨å·¦
    recent_30_days = recent_30_days[['Date', 'Daily_Flow_kg', 'Daily_Flow_l', 'Daily_Flow_sccm','Cumulative_Flow_converted']]
    recent_30_days.sort_values('Date', ascending=False, inplace=True)
    table_data = recent_30_days.set_index('Date').T

    # æ¨™è¨˜å‰äº”å¤§æ¯æ—¥ç”¨é‡
    top_5_dates = recent_30_days.nlargest(5, 'Daily_Flow_kg')['Date']
    highlight_cols = {date: 'background-color: yellow' for date in top_5_dates}

    # ä½¿ç”¨è‡ªå®šç¾©æ¨£å¼é«˜äº®å‰äº”å¤§æ—¥æœŸ
    def highlight_columns(data):
        return pd.DataFrame(
            [[highlight_cols.get(col, '') for col in data.columns]] * data.shape[0],
            columns=data.columns,
            index=data.index
        )

    # åœ¨ Streamlit ä¸­å±•ç¤ºæ ¼å¼åŒ–è¡¨æ ¼
    st.write("æœ€è¿‘ 30 å¤©è¡¨æ ¼ (åŒ…æ‹¬ä¸åŒæŒ‡æ¨™):")
    st.dataframe(table_data.style.apply(highlight_columns, axis=None))

    # æ¯æ—¥ç”¨é‡èˆ‡ç´¯ç©æµé‡çš„åœ–è¡¨
    st.header("æ¯æ—¥èˆ‡ç´¯ç©æµé‡åœ–è¡¨")
    fig, ax1 = plt.subplots(figsize=(12, 8))

    # é¡¯ç¤ºæ•¸æ“š
    daily_flow_kg = (filtered_daily_flow['Daily_Flow'] * SCCM_TO_KG).round(2)
    ax1.plot(filtered_daily_flow['Date'], daily_flow_kg, label=f"æ¯æ—¥ç”¨é‡ (kg/s)", color='blue', marker='o')
    ax2 = ax1.twinx()
    cumulative_flow_kg = (filtered_daily_flow['Cumulative_Flow'] * SCCM_TO_KG).round(2)
    ax2.bar(filtered_daily_flow['Date'], cumulative_flow_kg, label=f"ç´¯ç©æµé‡ (kg/s)", color='red', alpha=0.5)

    # ç¹ªè£½æ¯æ—¥ç”¨é‡çš„è¶¨å‹¢ç·š
    ax1.plot(filtered_daily_flow['Date'], (filtered_daily_flow['Daily_Flow'] * SCCM_TO_KG).round(2), label=f"æ¯æ—¥ç”¨é‡ {unit_label}", color='tab:blue', marker='o')
    ax1.plot(filtered_daily_flow['Date'], (filtered_daily_flow['7-Day MA'] * SCCM_TO_KG).round(2), label='7 å¤©ç§»å‹•å¹³å‡', color='orange', linestyle='--')
    ax1.plot(filtered_daily_flow['Date'], (filtered_daily_flow['30-Day MA'] * SCCM_TO_KG).round(2), label='30 å¤©ç§»å‹•å¹³å‡', color='green', linestyle='--')
    for i, v in enumerate((filtered_daily_flow['Daily_Flow'] * SCCM_TO_KG).round(2)):
        ax1.text(filtered_daily_flow['Date'].iloc[i], v, f'{v:.2f}', color='tab:blue', ha='center')
    ax1.set_xlabel("æ—¥æœŸ")
    ax1.set_ylabel(f"æ¯æ—¥ç”¨é‡ {unit_label}", color='tab:blue')
    ax1.tick_params(axis='y', labelcolor='tab:blue')

    # æ ¼å¼åŒ– X è»¸
    ax1.xaxis.set_major_locator(mdates.DayLocator(interval=1))
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha="right")

    # ä½¿ç”¨ç›´æ¢åœ–è¡¨ç¤ºç´¯ç©æµé‡
    ax2 = ax1.twinx()
    ax2.bar(filtered_daily_flow['Date'], (filtered_daily_flow['Cumulative_Flow'] * SCCM_TO_KG).round(2), label=f"ç´¯ç©æµé‡ {unit_label}", color='tab:red', alpha=0.6)
    for i, v in enumerate((filtered_daily_flow['Cumulative_Flow'] * SCCM_TO_KG).round(2)):
        ax2.text(filtered_daily_flow['Date'].iloc[i], v, f'{v:.2f}', color='tab:red', ha='center')
    ax2.set_ylabel(f"ç´¯ç©æµé‡ {unit_label}", color='tab:red')
    ax2.tick_params(axis='y', labelcolor='tab:red')

    # æ·»åŠ åœ–ä¾‹
    fig.tight_layout()
    ax1.legend(loc="upper left")
    ax2.legend(loc="upper right")

    # æ·»åŠ åœ–ä¾‹èˆ‡é¡¯ç¤º
    fig.tight_layout()
    st.pyplot(fig)

    # æˆæœ¬èˆ‡æ’æ”¾è¨ˆç®—
    st.title("NF3 å–®åƒ¹è¼¸å…¥")

    # ä½¿ç”¨è€…è¼¸å…¥ NF3 æ¯å…¬æ–¤çš„å–®åƒ¹
    NF3_price_per_kg = st.number_input(
        label="è¼¸å…¥æ¯å…¬æ–¤ NF3 çš„åƒ¹æ ¼:",
        min_value=0.0,  # è¨­å®šæœ€ä½å€¼
        value=50.0,     # é è¨­å€¼
        step=0.1        # æ¯æ¬¡èª¿æ•´çš„æ­¥é•·
    )

    # é¡¯ç¤ºä½¿ç”¨è€…è¼¸å…¥çš„å€¼
    st.write(f"æ¯å…¬æ–¤ NF3 çš„åƒ¹æ ¼ç‚º: ${NF3_price_per_kg:.2f}")

    filtered_daily_flow['Daily_Cost'] = filtered_daily_flow['Daily_Flow'] * NF3_price_per_kg
    filtered_daily_flow['GHG_Emissions'] = filtered_daily_flow['Daily_Flow'] * 17200  # NF3 GWP ç‚º 17200

 # æˆæœ¬èˆ‡æ’æ”¾æ‘˜è¦
    st.markdown("### æˆæœ¬èˆ‡æ’æ”¾åˆ†æ")
    st.write(f"ç¸½æˆæœ¬: ${filtered_daily_flow['Daily_Cost'].sum():,.2f}")
    st.write(f"ç¸½æº«å®¤æ°£é«”æ’æ”¾é‡: {filtered_daily_flow['GHG_Emissions'].sum():,.2f} kg CO2e")

    
 # GLASSID èˆ‡ PRODUCT çš„åˆ†æ
    st.header("GLASSID èˆ‡ PRODUCT åˆ†æ")
    data['GLASSID_prefix'] = data['GLASSID'].astype(str).str[:4]
    data['PRODUCT_prefix'] = data['PRODUCT'].astype(str).str[:4]
    
    # è¨ˆç®— GLASSID èˆ‡ PRODUCT çš„åŒ¹é…æ•¸é‡
    filtered_data = data[data['GLASSID_prefix'] == data['PRODUCT_prefix']]
    glassid_count = filtered_data['GLASSID'].nunique()
    st.write(f"GLASSID èˆ‡ PRODUCT å‰å››ç¢¼åŒ¹é…çš„ç»ç’ƒç‰‡æ•¸é‡: {glassid_count}")
    
    # å–®ç‰‡ç»ç’ƒå¹³å‡ NF3 æ¶ˆè€—
    average_nf3_per_glass = filtered_data.groupby(['GLASSID', 'TSTAMP'])['NF3_total_Flow'].mean().reset_index()
    average_nf3_per_glass.columns = ['GLASSID', 'Date', 'Average_NF3_Consumption']
    st.subheader("å–®ç‰‡ç»ç’ƒå¹³å‡ NF3 æ¶ˆè€—é‡ï¼ˆæ©«å‘çµ±è¨ˆï¼‰")
    pivot_glass_table = average_nf3_per_glass.pivot(index='GLASSID', columns='Date', values='Average_NF3_Consumption')
    st.dataframe(pivot_glass_table.style.format("{:.2f}"))
    
    # ç¸½ NF3 æ¶ˆè€—é‡
    total_nf3_consumption = filtered_data['NF3_total_Flow'].sum()
    nf3_per_glass = total_nf3_consumption / glassid_count if glassid_count > 0 else 0
    st.write(f"ç¸½ NF3 æ¶ˆè€—é‡: {total_nf3_consumption:.2f}")
    st.write(f"å–®ç‰‡ç»ç’ƒå¹³å‡ NF3 æ¶ˆè€—é‡: {nf3_per_glass:.2f}")
    
    # GLASSID èˆ‡ RPSC çš„åˆ†æ
    st.header("GLASSID èˆ‡ RPSC åˆ†æ")
    data['RECIPEID_prefix'] = data['RECIPEID'].astype(str).str[:4]
    filtered_rpsc_data = data[data['GLASSID_prefix'] == data['RECIPEID_prefix']]
    rpsc_glassid_count = filtered_rpsc_data['GLASSID'].nunique()
    st.write(f"GLASSID èˆ‡ RECIPEID å‰å››ç¢¼åŒ¹é…çš„ç»ç’ƒç‰‡æ•¸é‡ (RPSC Count): {rpsc_glassid_count}")
    
    # å–®æ¬¡ RPSC å¹³å‡ NF3 æ¶ˆè€—
    average_nf3_per_rpsc = filtered_rpsc_data.groupby(['GLASSID', 'TSTAMP', 'CHAMBERID'])['NF3_total_Flow'].mean().reset_index()
    average_nf3_per_rpsc.columns = ['GLASSID', 'Date', 'CHAMBERID', 'Average_NF3_Consumption']
    
    st.subheader("å–®æ¬¡ RPSC å¹³å‡ NF3 æ¶ˆè€—é‡ï¼ˆæŒ‰æ—¥æœŸèˆ‡ CHAMBERIDï¼‰")
    pivot_rpsc_table = average_nf3_per_rpsc.pivot(index='CHAMBERID', columns='Date', values='Average_NF3_Consumption')
    st.dataframe(pivot_rpsc_table.style.format("{:.2f}"))
    
    # ç¸½ RPSC NF3 æ¶ˆè€—é‡
    total_nf3_consumption_rpsc = filtered_rpsc_data['NF3_total_Flow'].sum()
    nf3_per_rpsc = total_nf3_consumption_rpsc / rpsc_glassid_count if rpsc_glassid_count > 0 else 0
    st.write(f"ç¸½ RPSC NF3 æ¶ˆè€—é‡: {total_nf3_consumption_rpsc:.2f}")
    st.write(f"å–®æ¬¡ RPSC å¹³å‡ NF3 æ¶ˆè€—é‡: {nf3_per_rpsc:.2f}")

    
if __name__ == "__main__":
    main()